# Customer Support Chatbot with RAG - Architecture Guide

## ğŸ¯ Overview

This document explains the architecture of a **Retrieval-Augmented Generation (RAG) based chatbot** that runs completely locally without any cloud services or paid APIs.

---

## ğŸ“š What is RAG (Retrieval-Augmented Generation)?

### The Problem Without RAG
Without RAG, LLMs have limitations:
- **Hallucinations**: Generate plausible-sounding but false information
- **Outdated Knowledge**: Trained data becomes stale
- **No Private Data Access**: Can't use your specific knowledge base
- **No Source Attribution**: Can't tell where information came from

### How RAG Solves This

RAG works in 3 simple steps:

```
1. RETRIEVAL  â†’ Find relevant documents from your knowledge base
2. CONTEXT    â†’ Combine retrieved text with the user's question
3. GENERATION â†’ Let the LLM answer using ONLY that context
```

**Result**: Grounded answers that cite sources and avoid hallucinations.

---

## ğŸ—ï¸ System Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERACTION LAYER                       â”‚
â”‚                    (Web UI / CLI / API)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   1. QUERY EMBEDDING                             â”‚
â”‚              (Convert text to vector)                            â”‚
â”‚      Model: Sentence-Transformers (all-MiniLM-L6)               â”‚
â”‚      Input: "What is your return policy?"                        â”‚
â”‚      Output: [0.23, -0.45, 0.67, ...] (384 dimensions)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 2. VECTOR DATABASE SEARCH                        â”‚
â”‚             (Find similar documents in DB)                       â”‚
â”‚         Database: FAISS or ChromaDB (local files)                â”‚
â”‚         Similarity Metric: Cosine Similarity                     â”‚
â”‚         Returns: Top-K most relevant chunks (k=5)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            3. CONTEXT CONSTRUCTION                               â”‚
â”‚        (Build the RAG prompt with context)                       â”‚
â”‚                                                                  â”‚
â”‚  Prompt Template:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ You are a helpful customer support bot.  â”‚                   â”‚
â”‚  â”‚                                          â”‚                   â”‚
â”‚  â”‚ Use ONLY this context to answer:        â”‚                   â”‚
â”‚  â”‚ [Retrieved document chunks]              â”‚                   â”‚
â”‚  â”‚                                          â”‚                   â”‚
â”‚  â”‚ Question: [User query]                   â”‚                   â”‚
â”‚  â”‚ Answer:                                  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            4. LLM RESPONSE GENERATION                            â”‚
â”‚              (Generate grounded answer)                          â”‚
â”‚    Models: Mistral / LLaMA 3 / Phi-3 (via Ollama)               â”‚
â”‚    Constraints: Only use provided context                        â”‚
â”‚    Output: Natural language response                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              5. RESPONSE FORMATTING                              â”‚
â”‚                                                                  â”‚
â”‚  {                                                               â”‚
â”‚    "question": "What is your return policy?",                   â”‚
â”‚    "answer": "We accept returns within 30 days...",             â”‚
â”‚    "confidence": "high",                                         â”‚
â”‚    "sources": [                                                  â”‚
â”‚      {"file": "policies.txt", "chunk_id": 3},                   â”‚
â”‚      {"file": "faq.json", "chunk_id": 1}                        â”‚
â”‚    ]                                                             â”‚
â”‚  }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Core Components

### 1. **Document Ingestion Module**
**File**: `src/modules/document_loader.py`

**What it does**:
- Loads documents from various sources (TXT, PDF, JSON, DOCX)
- Extracts text content
- Tracks document metadata

**Technologies**:
- PyPDF2 / pdfplumber (PDF files)
- python-docx (Word documents)
- json / csv (Structured data)

**Workflow**:
```
Raw Files â†’ Extract Text â†’ Store with Metadata
```

---

### 2. **Text Chunking Module**
**File**: `src/modules/text_chunker.py`

**What it does**:
- Splits long documents into chunks (500-1000 tokens)
- Maintains context by overlapping chunks
- Creates unique chunk IDs

**Why chunking matters**:
- LLMs have context limits
- Vector DBs work better with smaller, focused chunks
- Enables precise source attribution

**Example**:
```
Original: "Return Policy: Items can be returned within 30 days. 
Conditions: Must have receipt, tags attached, unworn/unused. 
Refunds processed in 5-7 business days."

Chunk 1: "Return Policy: Items can be returned within 30 days."
Chunk 2: "Conditions: Must have receipt, tags attached, unworn/unused."
Chunk 3: "Refunds processed in 5-7 business days."
```

---

### 3. **Embedding Module**
**File**: `src/modules/embeddings.py`

**What it does**:
- Converts text chunks into numerical vectors
- Uses sentence-level embeddings
- Stores embeddings with associated chunks

**Model**: `sentence-transformers/all-MiniLM-L6-v2`
- 384-dimensional vectors
- Fast (< 100ms per chunk)
- Excellent for semantic similarity

**How embeddings work**:
```
Text: "What is your refund policy?"
     â†“
[0.45, -0.23, 0.89, 0.12, ..., 0.34] (384 numbers)
     â†‘
These numbers capture semantic meaning
"refund" and "return" will have similar vectors
```

---

### 4. **Vector Database Module**
**File**: `src/modules/vector_database.py`

**What it does**:
- Stores embeddings and chunks
- Enables fast similarity search
- Persists data locally

**Options**:
- **FAISS** (Facebook AI Similarity Search)
  - Ultra-fast, standalone
  - Perfect for laptops
  - No server needed
  
- **ChromaDB**
  - More feature-rich
  - Better for production
  - Easier metadata handling

**Storage**: Local `.faiss` files (no cloud upload)

---

### 5. **Query Processing Module**
**File**: `src/modules/query_processor.py`

**What it does**:
1. Embeds user query (same model as chunks)
2. Searches vector DB for top-K similar chunks
3. Ranks results by relevance
4. Assembles context for LLM

**Configuration**:
```python
k = 5  # Return top-5 chunks
threshold = 0.6  # Minimum relevance score
```

---

### 6. **Response Generation Module**
**File**: `src/modules/response_generator.py`

**What it does**:
- Constructs RAG prompt
- Calls local LLM (via Ollama)
- Formats response with confidence score
- Tracks sources

**LLM Options**:
- **Mistral 7B**: Fast, accurate, good for RAG
- **LLaMA 3 8B**: Better quality, more resources
- **Phi-3 3.8B**: Ultra-lightweight, CPU-friendly

---

## ğŸ—‚ï¸ Project Structure

```
customer-support-chatbot/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                      # Configuration settings
â”‚   â”‚
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_loader.py         # Load documents
â”‚   â”‚   â”œâ”€â”€ text_chunker.py            # Chunk documents
â”‚   â”‚   â”œâ”€â”€ embeddings.py              # Generate embeddings
â”‚   â”‚   â”œâ”€â”€ vector_database.py         # FAISS/ChromaDB wrapper
â”‚   â”‚   â”œâ”€â”€ query_processor.py         # Retrieve relevant chunks
â”‚   â”‚   â””â”€â”€ response_generator.py      # Generate LLM responses
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py                  # Logging setup
â”‚   â”‚   â””â”€â”€ helpers.py                 # Utility functions
â”‚   â”‚
â”‚   â””â”€â”€ app.py                         # Main application
â”‚
â”œâ”€â”€ knowledge_base/
â”‚   â”œâ”€â”€ faqs/
â”‚   â”‚   â”œâ”€â”€ general_faq.json
â”‚   â”‚   â””â”€â”€ product_faq.json
â”‚   â”‚
â”‚   â””â”€â”€ documents/
â”‚       â”œâ”€â”€ return_policy.txt
â”‚       â”œâ”€â”€ shipping_info.pdf
â”‚       â””â”€â”€ product_manual.docx
â”‚
â”œâ”€â”€ vector_store/                      # Vector DB storage
â”‚   â”œâ”€â”€ faiss_index.index
â”‚   â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ logs/                              # Application logs
â”‚   â””â”€â”€ chatbot.log
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.sh / setup.bat               # Installation script
â”œâ”€â”€ ARCHITECTURE.md                    # This file
â”œâ”€â”€ SETUP_GUIDE.md                     # Installation guide
â”œâ”€â”€ USAGE_GUIDE.md                     # How to use
â””â”€â”€ README.md                          # Quick start
```

---

## ğŸ’¾ Data Flow: Initialization Phase

When you first run the chatbot:

```
1. LOAD DOCUMENTS
   knowledge_base/ 
   â†’ document_loader.py
   â†’ Extract all text

2. CHUNK TEXT
   Raw text (10,000 words)
   â†’ text_chunker.py
   â†’ 15 chunks (500-800 words each)

3. GENERATE EMBEDDINGS
   Each chunk
   â†’ Sentence-Transformers
   â†’ 384-dimensional vector

4. STORE IN VECTOR DB
   Chunks + Embeddings
   â†’ FAISS index
   â†’ vector_store/faiss_index.index
```

**One-time operation** - happens during initialization, then reused.

---

## ğŸ’¬ Data Flow: Query Phase

When user asks a question:

```
1. USER QUERY
   "What's your return policy?"

2. EMBED QUERY
   â†’ Same embedding model
   â†’ [0.45, -0.23, 0.89, ...] (384 dims)

3. SEARCH VECTOR DB
   Query vector
   â†’ Cosine similarity search
   â†’ Top-5 chunks with scores:
      â”œâ”€ Chunk #3 (score: 0.92)
      â”œâ”€ Chunk #7 (score: 0.88)
      â”œâ”€ Chunk #2 (score: 0.85)
      â”œâ”€ Chunk #9 (score: 0.79)
      â””â”€ Chunk #1 (score: 0.76)

4. BUILD CONTEXT
   System Prompt:
   "You are a helpful customer support bot.
    Answer ONLY using the provided context.
    
    Context:
    [Retrieved chunks combined]
    
    Question: What's your return policy?
    
    Answer:"

5. CALL LOCAL LLM (Ollama)
   Prompt â†’ Mistral 7B
   â†“
   "We accept returns within 30 days..."

6. RETURN RESPONSE
   {
     "question": "What's your return policy?",
     "answer": "We accept returns within...",
     "confidence": "high",
     "sources": ["policies.txt:chunk_3", "faq.json:chunk_7"]
   }
```

---

## ğŸ› ï¸ Technology Choices Explained

### Why Sentence-Transformers?
- **Semantic Understanding**: Understands meaning, not just keywords
- **Lightweight**: 384 dimensions vs 4096+ for large models
- **Fast**: CPU inference < 100ms
- **Free**: Open-source, no API costs

### Why FAISS?
- **Speed**: Searches billions of vectors in milliseconds
- **Offline**: Works locally without internet
- **Scalable**: Handles 1M+ vectors on laptop
- **Minimal Dependencies**: Pure C++ under the hood

### Why Ollama?
- **Local LLM Running**: Simplest way to run models locally
- **GPU/CPU Flexible**: Works on any hardware
- **Model Manager**: Easy to switch between models
- **No Configuration**: Just run `ollama run mistral`

---

## ğŸ¯ Key Concepts Summary

| Concept | Explanation | Benefit |
|---------|-------------|---------|
| **Embedding** | Text converted to numbers (vector) | Enables similarity search |
| **Vector DB** | Database of embeddings | Fast retrieval of similar docs |
| **RAG Prompt** | Template including context + question | Grounds LLM responses |
| **Chunking** | Breaking docs into pieces | Better retrieval accuracy |
| **Cosine Similarity** | Measure of vector closeness (0-1) | Relevance scoring |
| **Top-K Retrieval** | Return top 5 most similar chunks | Balance quality & speed |
| **Confidence Score** | How certain the answer is | User trust indicator |

---

## ğŸš€ Performance Characteristics

### Latency (per query)
- Embedding query: **10-50ms**
- Vector DB search: **5-20ms**
- LLM generation: **1-5 seconds** (depends on answer length)
- **Total: 1-5.5 seconds** per query

### Memory Usage
- Embedding model: **100-200 MB**
- Vector DB (1000 chunks): **50-100 MB**
- LLM model in memory: **3-16 GB** (depends on model size)
- **Total: 3.2-16.3 GB**

### Storage
- Models: **3-15 GB** (one-time download)
- Vector index: **100 MB** (per 10,000 chunks)
- Knowledge base: **50-500 MB** (depends on docs)

### Laptop Compatibility
âœ… Works on: 
- **8GB RAM** (with Phi-3 3.8B)
- **16GB RAM** (with Mistral 7B or LLaMA 3 8B)
- **CPU-only laptops** (Intel i5+ / AMD Ryzen 5+)
- **With GPU**: 2-4x faster

---

## ğŸ” Security & Privacy

### Data Stays Local
âœ… No data sent to cloud
âœ… No API calls to external services
âœ… Complete privacy for sensitive docs
âœ… GDPR/compliance friendly

### No Model Training
âœ… Uses pre-trained models
âœ… Your docs don't modify models
âœ… Safe for confidential information

---

## Next Steps

1. **Setup**: Follow [SETUP_GUIDE.md](SETUP_GUIDE.md)
2. **Run**: Follow [USAGE_GUIDE.md](USAGE_GUIDE.md)
3. **Customize**: Modify prompts in [src/config.py](src/config.py)
4. **Deploy**: See production tips in [OPTIMIZATION.md](OPTIMIZATION.md)

---

**Questions?** See detailed implementation in the code files or refer to the API documentation.
